강의내용1.txt

트리의 앙상블
키워드로 정리
1. 앙상블 학습
2. 랜덤 포레스트
3. 엑스트라 트리
4. 그레디언트 부스팅

핵심 패키지와 함수
1. scikit-learn
1) RandomForestClassifier
2) ExtraTreesClassifier
3) GradientBoostingClassifier
4) HistGradientBoostingClassifier

정형 데이터와 비정형 데이터
1. 정형 데이터
1) 어떤 구조로 되어 있는 데이터
2) 이런 데이터는 CSV나 데이터베이스(Database), 혹은 엑셀(Excel)에 저장하기 쉽습니다.
3) 프로그래머가 다루는 대부분의 데이터가 정형 데이터 입니다.
4) 지금까지 배운 머신러닝 알고리즘은 정형 데이터에 잘 맞습니다.
5) 정형 데이터를 다루는데 가장 뛰어난 성과를 내는 알고리즘이 앙상블 학습(ensemble learning)입니다.
6) 이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있습니다.

2. 비정형 데이터
1) 데이터베이스나 엑셀로 표현하기 어려운 것들
2) 책의 글과 같은 텍스트 데이터, 디지털카메라로 찍은 사진, 핸드폰으로 듣는 디지털 음악 등
3) 비정형 데이터는 규칙성을 찾기 어려워 전통적인 머신러닝 방법으로는 모델을 만들기 까다롭습니다.
4) 신경망 알고리즘의 놀라운 발전 덕분에 사진을 인식하고 텍스트를 이해하는 모델을 만들 수 있습니다.

랜덤 포레스트
1. 부트스트랩 샘플 사용 
-  데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식
2. 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾습니다
3. 분류 모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택합니다. 즉 4개의 특성이 있다면 노드마다 2개를 랜덤하게 선택하여 사용합니다.

엑스트라트리
1. 부트스트랩 샘플을 사용하지 않고 결정 트리를 만들때 전체 훈련세트를 사용
2. 노드를 분할할 때 가장 좋은 분할을 찾는 것이 어나러 무작위로 분할합니다.

그레디언트 부스팅
1. 깊이가 얖은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블하는 방법
2. 기본적으로 깊이가 3인 결정 트리를 100개 사용합니다.
3. 깊이가 얖은 결정트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있습니다.
4. 그레이디언트란 이름과 같이 경사 하강법을 사용하여 트리를 앙상블에 추가합니다. 분류에서는 로지스틱 손실 함수를 사용하고 회귀에서는 평균 제곱 오차 함수를 사용합니다.

히스토그램 기반 부스팅
1. 히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘입니다.
2. 히스토그램 기반 그레디언트 부스팅은 먼저 입력 특성을 256개의 구간으로 나눕니다. 따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있습니다.
3. 히스토그램 기반 그레이디언트 부스팅은 256개의 구간 중에서 하나를 떼어 노고 누락된 값을 위해서 사용합니다. 따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없습니다.
========================
